{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import optimizers\n",
    "from keras.layers.recurrent import LSTM\n",
    "from keras.layers import Reshape, Lambda, BatchNormalization\n",
    "import keras_preprocessing\n",
    "from keras_preprocessing.image import ImageDataGenerator\n",
    "from keras.applications.resnet_v2 import preprocess_input\n",
    "import pandas as pd\n",
    "import os\n",
    "import keras\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.applications.resnet50 import ResNet50\n",
    "from keras.layers import Dense, Input, Activation\n",
    "from keras.models import Model\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "\n",
    "tf.debugging.set_log_device_placement(True)\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    try:\n",
    "        for gpu in gpus:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "\n",
    "    except RuntimeError as e:\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dir=os.path.join('./train_under/')\n",
    "val_dir=os.path.join('./val/')\n",
    "test_dir=os.path.join('./test/')\n",
    "train_datagen=ImageDataGenerator(rescale=1/255)\n",
    "train_generator=train_datagen.flow_from_directory(train_dir,\n",
    "                                              target_size=(224,224),\n",
    "                                              batch_size=32,\n",
    "                                              class_mode='categorical')\n",
    "test_datagen=ImageDataGenerator(rescale=1/255)\n",
    "test_generator=train_datagen.flow_from_directory(test_dir,\n",
    "                                                  target_size=(224,224),\n",
    "                                                  batch_size=32,\n",
    "                                                  class_mode='categorical')\n",
    "val_datagen=ImageDataGenerator(rescale=1/255)\n",
    "val_generator=train_datagen.flow_from_directory(val_dir,\n",
    "                                                  target_size=(224,224),\n",
    "                                                  batch_size=32,\n",
    "                                                  class_mode='categorical')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model\n",
    "#Resnet model\n",
    "input = Input(shape=(224, 224, 3))\n",
    "model=ResNet50(\n",
    "    weights=\"imagenet\",\n",
    "    input_tensor=input,\n",
    "    pooling='max',\n",
    "    include_top=False,\n",
    ")\n",
    "x = model.output\n",
    "x = Dense(1024, name='fully', kernel_initializer='random_uniform')(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = Activation('relu')(x)\n",
    "x = Dense(512, kernel_initializer='random_uniform')(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = Activation('relu')(x)\n",
    "x = Dense(128)(x)\n",
    "x = Reshape((1,128))(x)\n",
    "x = LSTM(128, return_sequences=True, kernel_initializer='he_normal', name='lstm1')(x)\n",
    "x = Dense(2, kernel_initializer='he_normal',name='dense1')(x)\n",
    "x = Activation('softmax', name='softmax')(x)\n",
    "model = Model(model.input, x)\n",
    "model.summary()\n",
    "sgd = optimizers.SGD(lr=0.01, decay=1e-4, momentum=0.9, nesterov=True)\n",
    "\n",
    "model.compile(optimizer=sgd,\n",
    "            loss='binary_crossentropy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scheduler(epoch, lr):\n",
    "    if epoch < 20:\n",
    "        return lr\n",
    "    else:\n",
    "        return lr * 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Tuple\n",
    "from keras.callbacks import Callback\n",
    "from sklearn.metrics import classification_report\n",
    "import numpy as np\n",
    "import itertools\n",
    "\n",
    "class Metrics(Callback):\n",
    "\n",
    "    def __init__(self, val_data,batch_size: int = 32):\n",
    "        super().__init__()\n",
    "        self.validation_data = val_data\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        val_true, val_pred = evaluate(data_gen=self.validation_data, model=self.model)\n",
    "        val_true_class = np.argmax(val_true, axis=1)\n",
    "        val_pred_class = np.argmax(val_pred, axis=1)\n",
    "\n",
    "        cls_report = classification_report(\n",
    "            y_true=val_true_class,\n",
    "            y_pred=val_pred_class,\n",
    "            output_dict=True,\n",
    "            target_names=['0','1'],\n",
    "            labels=np.arange(2)        )\n",
    "        with open('./weights/logs','a') as f:\n",
    "            f.write(str(epoch)+'         '+str(cls_report)+'\\n')\n",
    "        print(cls_report)\n",
    "\n",
    "\n",
    "\n",
    "def evaluate(data_gen, model) -> Tuple[np.ndarray]:\n",
    "    \"\"\"\n",
    "    Uses a model to predict on all examples in a data generator.\n",
    "\n",
    "    Args:\n",
    "        data_gen: Keras data generator\n",
    "        model: Keras model\n",
    "        n_batches: int\n",
    "\n",
    "    Returns:\n",
    "        y_true: Ground truth labels. Shape (n_examples, )\n",
    "        y_pred: Predicted labels. Shape (n_examples, )\n",
    "\n",
    "    \"\"\"\n",
    "    y_preds = []\n",
    "    y_trues = []\n",
    "    for idx in range(len(data_gen)):\n",
    "        print('======================{}======================\\n'.format(idx))\n",
    "        X, y_true = next(data_gen)\n",
    "        y_trues.append(y_true)\n",
    "        y_pred = model.predict(X)\n",
    "        y_preds.extend(y_pred)\n",
    "\n",
    "    y_true = np.concatenate(y_trues, axis=0)\n",
    "    y_pred = np.concatenate(y_preds, axis=0)\n",
    "\n",
    "    return y_true, y_pred\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(action='ignore')\n",
    "\n",
    "with tf.device('/gpu:0'):\n",
    "    filename = './weights/weights-improvement-{epoch:02d}.hdf5'\n",
    "    checkpoint = ModelCheckpoint(filename,             \n",
    "                                 monitor='val_loss',   \n",
    "                                 verbose=1,           \n",
    "                                 save_best_only=False,\n",
    "                                 mode='auto' \n",
    "                                )\n",
    "    #training\n",
    "    history = model.fit_generator(train_generator,\n",
    "                                  epochs=60,\n",
    "                                  steps_per_epoch=len(train_generator),\n",
    "                                  validation_data=val_generator,\n",
    "                                  validation_steps=1,\n",
    "                                  callbacks=[Metrics(val_generator),\n",
    "                                            tf.keras.callbacks.LearningRateScheduler(scheduler),\n",
    "                                            checkpoint]\n",
    "                                 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import load_model\n",
    "model = load_model('./weights/weights-improvement-14.hdf5')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_evaluate(data_gen, model) -> Tuple[np.ndarray]:\n",
    "\n",
    "    y_preds = []\n",
    "    y_trues = []\n",
    "    for idx in range(len(data_gen)):\n",
    "        print('======================{}======================\\n'.format(idx))\n",
    "        X, y_true = next(data_gen)\n",
    "        y_trues.append(y_true)\n",
    "        y_pred = model.predict(X)\n",
    "        y_preds.extend(y_pred)\n",
    "\n",
    "    y_true = np.concatenate(y_trues, axis=0)\n",
    "    y_pred = np.concatenate(y_preds, axis=0)\n",
    "    \n",
    "    val_true, val_pred = y_true,y_pred\n",
    "    val_true_class = np.argmax(val_true, axis=1)\n",
    "    val_pred_class = np.argmax(val_pred, axis=1)\n",
    "\n",
    "    cls_report = classification_report(\n",
    "        y_true=val_true_class,\n",
    "        y_pred=val_pred_class,\n",
    "        output_dict=True,\n",
    "        target_names=['0','1'],\n",
    "        labels=np.arange(2)        )\n",
    "\n",
    "    print(cls_report)\n",
    "\n",
    "    return y_true, y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_evaluate(test_generator,model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#feature_extract\n",
    "train_dir=os.path.join('./train/')\n",
    "val_dir=os.path.join('./val/')\n",
    "test_dir=os.path.join('./test/')\n",
    "train_datagen=ImageDataGenerator(rescale=1/255)\n",
    "train_generator=train_datagen.flow_from_directory(train_dir,\n",
    "                                              target_size=(224,224),\n",
    "                                              batch_size=32,\n",
    "                                              class_mode='categorical')\n",
    "test_datagen=ImageDataGenerator(rescale=1/255)\n",
    "test_generator=train_datagen.flow_from_directory(test_dir,\n",
    "                                                  target_size=(224,224),\n",
    "                                                  batch_size=32,\n",
    "                                                  class_mode='categorical')\n",
    "val_datagen=ImageDataGenerator(rescale=1/255)\n",
    "val_generator=train_datagen.flow_from_directory(val_dir,\n",
    "                                                  target_size=(224,224),\n",
    "                                                  batch_size=32,\n",
    "                                                  class_mode='categorical')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#feature extract\n",
    "fe_model = Model(inputs=model.input, outputs=model.get_layer('lstm1').output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from tensorflow.keras.preprocessing import image\n",
    "import numpy as np\n",
    "with open('./train_val_test','r') as f:\n",
    "    data_split=json.load(f)\n",
    "data=pd.read_csv('./new_metadata_real.csv')\n",
    "\n",
    "train=data_split['train']\n",
    "val=data_split['val']\n",
    "test=data_split['test']\n",
    "\n",
    "test_sample={}\n",
    "for i in test:\n",
    "    print(i)\n",
    "    for j in os.listdir('Games/'):\n",
    "        if i in j:\n",
    "            st=int(data[data['gameid']==int(i)]['new_st'])\n",
    "            dur=int(data[data['gameid']==int(i)]['new_et'])-int(st)\n",
    "            sample=[0]*(dur+1)\n",
    "            file=os.listdir('Games/'+j)\n",
    "            for f in file:\n",
    "                try:\n",
    "                    index=int(f[:-4])-st\n",
    "                    img = image.load_img('Games/'+j+'/'+f, target_size=(224, 224))\n",
    "                    x = image.img_to_array(img)\n",
    "                    x /=255\n",
    "                    x=np.expand_dims(x, axis=0)\n",
    "                    feature=fe_model(x)\n",
    "                    feature_l=(list(np.array(feature).reshape(128)))\n",
    "                    sample[int(index)]=feature_l\n",
    "                except:\n",
    "                    print(i,f)\n",
    "\n",
    "            test_sample[i]=sample\n",
    "\n",
    "#                 except:\n",
    "#                     print(i, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py140",
   "language": "python",
   "name": "py140"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
